# -*- coding: utf-8 -*-
"""diyabetProjectAI

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_JXw6Yus8etyuIwPJmgBWhMkTC_lCi9w
"""

#hado homa 4 bibs l asasiyin l aya project
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#bach n9sam data l 2 parts wahad train o lakhor tst
from sklearn.model_selection import train_test_split

#algorithms li ghadi i trainiw lina 3la data
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier

#hado homa l metrics li ghadi n9iss bihom chhal kafa2a dial 4 li fo9hom
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score , recall_score , f1_score
from sklearn.metrics import classification_report

#bach nwazn gal data diali dengesiz
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler

#kanchofo rslt l 9dima ki kant o hta jdida
from collections import Counter

plt.style.use('fivethirtyeight')

import warnings
warnings.filterwarnings('ignore')

#kan9ra data diali
dataset = pd.read_csv(r'/content/sample_data/diabetes.csv')

#bach nchof data diali (had l 9aws. bach tla3 dakchi n9i9i - kon madrtouch ghadi tla3 lia bkharb9a bhal li writ l yuba)
dataset.head(1000)

#charha rassha
dataset.info()

dataset.describe()

#hna chft wach kayna chi haja mdoubla o dik .sum bach yjma3ha lia
dataset.duplicated().sum

"""# *Analysis*"""

#l3ala9a ola irtibat bin l3amdan
 dataset.corr()

#nfs li lfo9 ms bayn ktar o dik annot katzid hta rakm machi gha loun
sns.heatmap(dataset.corr(),annot = True , fmt="0.1f" , linewidth=.5)

#bach nchof chhal mn wahad mdir o chhal la
sns.countplot(x="Outcome" , data=dataset)

max=dataset[dataset["Outcome"]==0]
min=dataset[dataset["Outcome"]==1]
max.shape , min.shape
268/(500+268)

"""# Creat **Model**"""

#9assamna data bin x o y - ga3 data x illa saf lakhar ila y
x=dataset.drop("Outcome", axis = 1) #ga3 data illa outcome
y=dataset["Outcome"]

#katabat nisba data li kayakhodha kol mara doc accurcy makatbadalch
#dengasiz problemi
rm=RandomOverSampler(random_state=41)
x_res , y_res = rm.fit_resample(x,y)

print("old dataset shape{}" .format(Counter(y)))
print("old dataset shape{}" .format(Counter(y_res)))

#9assamna l x-y.train o x-y.test b function li derna l fo9 dial train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=.2) #.2 hia 20%

model1=LogisticRegression()
model2=SVC()
model3=RandomForestClassifier(n_estimators=100 , class_weight="balanced")
model4=GradientBoostingClassifier(n_estimators=1000)

col=["LogisticRegression ,SVC,RandomForestClassifier,GradientBoostingClassifier"]
result1=[]
result2=[]
result3=[]

"""# nsaybo function li smitha **model**"""

def cal(model): #bach nbda n3ayat 3la model o ydir lia hadchi li lta7t

  #kangolih ytrini lia 3la data li f x_train o y_train
  model.fit(x_train,y_train)

  #mora matrina db ytnaba2 b natija dial x
  pre=model.predict(x_test)

  #nchof tawa9o3 dalo chhal s7i7 parrapot l y
  accuracy=accuracy_score(pre,y_test)
  recall=recall_score(pre,y_test)
  f1=f1_score(pre,y_test)

  result1.append(accuracy)
  result2.append(recall)
  result3.append(f1)


  # hna ghadi i3tina output fiha tp tn fp fn
  sns.heatmap(confusion_matrix(pre,y_test),annot= True)

  print(model)
  print("accuracy is :",accuracy)
  print("recall is :", recall)
  print("f1 is :",f1)

cal(model1)

cal(model2)

cal(model3)

cal(model4)

col

FinalResult=pd.DataFrame({"accuricies":result1,"recall":result2,"fScore":result3})
FinalResult